{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import TUDataset, Planetoid, LRGBDataset\n",
    "from torch_geometric.transforms import NormalizeFeatures, Constant, OneHotDegree\n",
    "from torch_geometric.nn import GCNConv, GINConv, GATv2Conv, GPSConv, global_mean_pool, Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cora: Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "====================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "=============================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Number of training graphs: 2708\n",
      "Number of test graphs: 2708\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='.', name='Cora', transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "train_dataset = data.train_mask\n",
    "test_dataset = data.test_mask\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# stats about the first graph\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "Epoch: 000, Train Acc: 0.4500, Test Accuracy: 0.2400\n",
      "Epoch: 010, Train Acc: 0.8786, Test Accuracy: 0.5540\n",
      "Epoch: 020, Train Acc: 0.9429, Test Accuracy: 0.6850\n",
      "Epoch: 030, Train Acc: 0.9500, Test Accuracy: 0.7130\n",
      "Epoch: 040, Train Acc: 0.9571, Test Accuracy: 0.7350\n",
      "Epoch: 050, Train Acc: 0.9643, Test Accuracy: 0.7630\n",
      "Training GIN\n",
      "Epoch: 000, Train Acc: 0.1429, Test Accuracy: 0.1490\n",
      "Epoch: 010, Train Acc: 0.2714, Test Accuracy: 0.2990\n",
      "Epoch: 020, Train Acc: 0.5429, Test Accuracy: 0.4780\n",
      "Epoch: 030, Train Acc: 0.8143, Test Accuracy: 0.5790\n",
      "Epoch: 040, Train Acc: 0.9714, Test Accuracy: 0.6410\n",
      "Epoch: 050, Train Acc: 0.9929, Test Accuracy: 0.6470\n",
      "Training GATv2\n",
      "Epoch: 000, Train Acc: 0.1500, Test Accuracy: 0.1330\n",
      "Epoch: 010, Train Acc: 0.8143, Test Accuracy: 0.5600\n",
      "Epoch: 020, Train Acc: 0.9214, Test Accuracy: 0.7570\n",
      "Epoch: 030, Train Acc: 0.9500, Test Accuracy: 0.7490\n",
      "Epoch: 040, Train Acc: 0.9571, Test Accuracy: 0.7550\n",
      "Epoch: 050, Train Acc: 0.9500, Test Accuracy: 0.7370\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='.', name='Cora', transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "train_mask = data.train_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GIN, self).__init__()\n",
    "        nn1 = torch.nn.Sequential(torch.nn.Linear(dataset.num_features, 16), torch.nn.ReLU(), torch.nn.Linear(16, 16))\n",
    "        nn2 = torch.nn.Sequential(torch.nn.Linear(16, 16), torch.nn.ReLU(), torch.nn.Linear(16, 16))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(dataset.num_features, 16)\n",
    "        self.conv2 = GATv2Conv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = torch.nn.functional.nll_loss(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(model, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=-1)\n",
    "    correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "    acc = correct / mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "hidden_channels = [64, 64, 64]\n",
    "models = [GCN(), GIN(), GATv2()]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(51):\n",
    "        loss = train(model)\n",
    "        train_acc = test(model, data.train_mask)\n",
    "        test_acc = test(model, data.test_mask)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Acc: 0.2071, Test Accuracy: 0.1310\n",
      "Epoch: 010, Train Acc: 0.2643, Test Accuracy: 0.1690\n",
      "Epoch: 020, Train Acc: 0.4429, Test Accuracy: 0.2830\n",
      "Epoch: 030, Train Acc: 0.8357, Test Accuracy: 0.5280\n",
      "Epoch: 040, Train Acc: 0.9143, Test Accuracy: 0.6170\n"
     ]
    }
   ],
   "source": [
    "class GPSConvNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1, dropout=0.0, act='relu'):\n",
    "        super(GPSConvNet, self).__init__()\n",
    "        self.GPSConvs = nn.ModuleList()\n",
    "        prev_channels = dataset.num_node_features\n",
    "        h = hidden_channels[0]\n",
    "        self.preprocess = nn.Sequential(Linear(dataset.num_node_features, 2 * h), nn.GELU(), Linear(2 * h, h), nn.GELU())\n",
    "        for h in hidden_channels:\n",
    "            gatv2_conv = GATv2Conv(h, h // heads, heads=heads, dropout=dropout)\n",
    "            gps_conv = GPSConv(h, gatv2_conv, heads=4, dropout=dropout, act=act)\n",
    "            self.GPSConvs.append(gps_conv)\n",
    "            prev_channels = h\n",
    "        self.final_conv = GATv2Conv(prev_channels, dataset.num_classes, heads=1, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.preprocess(x)\n",
    "        for gps_conv in self.GPSConvs:\n",
    "            x = x.float()\n",
    "            x = F.relu(gps_conv(x, edge_index))\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.final_conv(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "hidden_channels = [64, 64, 64]\n",
    "\n",
    "model = GPSConvNet(hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = torch.nn.functional.nll_loss(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(model, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=-1)\n",
    "    correct = pred[mask].eq(data.y[mask]).sum().item()\n",
    "    acc = correct / mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "for epoch in range(50):\n",
    "    loss = train()\n",
    "    train_acc = test(model, data.train_mask)\n",
    "    test_acc = test(model, data.test_mask)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enzyme: Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: ENZYMES(600):\n",
      "====================\n",
      "Number of graphs: 600\n",
      "Number of features: 3\n",
      "Number of classes: 6\n",
      "\n",
      "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 37\n",
      "Number of edges: 168\n",
      "Average node degree: 4.54\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Number of training graphs: 450\n",
      "Number of test graphs: 150\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='.', name='ENZYMES')\n",
    "data = dataset[0]\n",
    "\n",
    "train_dataset = dataset[:450]\n",
    "test_dataset = dataset[450:]\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "Epoch: 100, Train Acc: 0.2244, Test Acc: 0.1867\n",
      "Epoch: 200, Train Acc: 0.2667, Test Acc: 0.2333\n",
      "Epoch: 300, Train Acc: 0.2822, Test Acc: 0.2867\n",
      "Epoch: 400, Train Acc: 0.2778, Test Acc: 0.2867\n",
      "Epoch: 500, Train Acc: 0.2822, Test Acc: 0.3067\n",
      "Epoch: 600, Train Acc: 0.2756, Test Acc: 0.3067\n",
      "Epoch: 700, Train Acc: 0.2844, Test Acc: 0.3067\n",
      "Epoch: 800, Train Acc: 0.3222, Test Acc: 0.3200\n",
      "Epoch: 900, Train Acc: 0.2956, Test Acc: 0.3000\n",
      "Training GIN\n",
      "Epoch: 100, Train Acc: 0.2844, Test Acc: 0.2800\n",
      "Epoch: 200, Train Acc: 0.2600, Test Acc: 0.2867\n",
      "Epoch: 300, Train Acc: 0.3111, Test Acc: 0.2400\n",
      "Epoch: 400, Train Acc: 0.3044, Test Acc: 0.2733\n",
      "Epoch: 500, Train Acc: 0.3467, Test Acc: 0.3200\n",
      "Epoch: 600, Train Acc: 0.3422, Test Acc: 0.3200\n",
      "Epoch: 700, Train Acc: 0.3400, Test Acc: 0.2933\n",
      "Epoch: 800, Train Acc: 0.3333, Test Acc: 0.2867\n",
      "Epoch: 900, Train Acc: 0.3533, Test Acc: 0.3600\n",
      "Training GATv2\n",
      "Epoch: 100, Train Acc: 0.2511, Test Acc: 0.2200\n",
      "Epoch: 200, Train Acc: 0.2511, Test Acc: 0.2600\n",
      "Epoch: 300, Train Acc: 0.3556, Test Acc: 0.3867\n",
      "Epoch: 400, Train Acc: 0.3533, Test Acc: 0.3600\n",
      "Epoch: 500, Train Acc: 0.3511, Test Acc: 0.4000\n",
      "Epoch: 600, Train Acc: 0.3444, Test Acc: 0.3600\n",
      "Epoch: 700, Train Acc: 0.3644, Test Acc: 0.4067\n",
      "Epoch: 800, Train Acc: 0.3933, Test Acc: 0.3867\n",
      "Epoch: 900, Train Acc: 0.3844, Test Acc: 0.3600\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "dataset = TUDataset(root='.', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:450]\n",
    "test_dataset = dataset[450:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(dataset.num_node_features, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)       \n",
    "        return x\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "        self.lin = Linear(hidden_channels * heads, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x) \n",
    "        return x\n",
    "        \n",
    "\n",
    "gcn_model = GCN(hidden_channels=64)\n",
    "gin_model = GIN(hidden_channels=64)\n",
    "gatv2_model = GATv2(hidden_channels=64, heads=8)\n",
    "\n",
    "models = [gcn_model, gin_model, gatv2_model]\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "         out = model(data.x, data.edge_index, data.batch) \n",
    "         loss = criterion(out, data.y)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(model, loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     for data in loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)\n",
    "         correct += int((pred == data.y).sum())\n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        train(model)\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPSConvNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1, dropout=0.0, act='relu'):\n",
    "        super(GPSConvNet, self).__init__()\n",
    "        self.GPSConvs = nn.ModuleList()\n",
    "        h = hidden_channels[0]\n",
    "        self.preprocess = nn.Sequential(Linear(dataset.num_node_features, 2 * h), nn.GELU(), Linear(2 * h, h), nn.GELU())\n",
    "        for h in hidden_channels:\n",
    "            gatv2_conv = GATv2Conv(h, h // heads, heads=heads, dropout=dropout)\n",
    "            gps_conv = GPSConv(h, gatv2_conv, heads=4, dropout=dropout, act=act)\n",
    "            self.GPSConvs.append(gps_conv)  \n",
    "        self.final_lin = Linear(hidden_channels[-1], dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.preprocess(x)\n",
    "        for gps_conv in self.GPSConvs:\n",
    "            x = x.float()\n",
    "            x = F.relu(gps_conv(x, edge_index))\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.final_lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "hidden_channels = [64, 64, 64]\n",
    "dataset = TUDataset(root='.', name='ENZYMES')\n",
    "data = dataset[0]\n",
    "\n",
    "model = GPSConvNet(hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "train_dataset = dataset[:850]\n",
    "test_dataset = dataset[850:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         loss = criterion(out, data.y)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)\n",
    "         correct += int((pred == data.y).sum())\n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Training {model.__class__.__name__}\")\n",
    "# \n",
    "for epoch in range(1, 21):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB: Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: IMDB-BINARY(1000):\n",
      "====================\n",
      "Number of graphs: 1000\n",
      "Number of features: 0\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 146], y=[1], num_nodes=20)\n",
      "=============================================================\n",
      "Number of nodes: 20\n",
      "Number of edges: 146\n",
      "Average node degree: 7.30\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Number of training graphs: 850\n",
      "Number of test graphs: 150\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='.', name='IMDB-BINARY')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "train_dataset = dataset[:850]\n",
    "test_dataset = dataset[850:]\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "Epoch: 100, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 200, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 300, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 400, Train Acc: 0.6224, Test Acc: 0.5867\n",
      "Epoch: 500, Train Acc: 0.6259, Test Acc: 0.5600\n",
      "Epoch: 600, Train Acc: 0.6059, Test Acc: 0.5400\n",
      "Epoch: 700, Train Acc: 0.6071, Test Acc: 0.5467\n",
      "Epoch: 800, Train Acc: 0.6059, Test Acc: 0.5400\n",
      "Epoch: 900, Train Acc: 0.6259, Test Acc: 0.5533\n",
      "Epoch: 1000, Train Acc: 0.6071, Test Acc: 0.5600\n",
      "Training GIN\n",
      "Epoch: 100, Train Acc: 0.6329, Test Acc: 0.6400\n",
      "Epoch: 200, Train Acc: 0.6471, Test Acc: 0.6267\n",
      "Epoch: 300, Train Acc: 0.6565, Test Acc: 0.6333\n",
      "Epoch: 400, Train Acc: 0.6341, Test Acc: 0.6133\n",
      "Epoch: 500, Train Acc: 0.6376, Test Acc: 0.6133\n",
      "Epoch: 600, Train Acc: 0.5882, Test Acc: 0.5667\n",
      "Epoch: 700, Train Acc: 0.6365, Test Acc: 0.6133\n",
      "Epoch: 800, Train Acc: 0.6553, Test Acc: 0.6267\n",
      "Epoch: 900, Train Acc: 0.6682, Test Acc: 0.6400\n",
      "Epoch: 1000, Train Acc: 0.6094, Test Acc: 0.5867\n",
      "Training GATv2\n",
      "Epoch: 100, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 200, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 300, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 400, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 500, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 600, Train Acc: 0.4988, Test Acc: 0.5067\n",
      "Epoch: 700, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 800, Train Acc: 0.5012, Test Acc: 0.4933\n",
      "Epoch: 900, Train Acc: 0.4988, Test Acc: 0.5067\n",
      "Epoch: 1000, Train Acc: 0.5012, Test Acc: 0.4933\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "dataset = TUDataset(root='.', name='IMDB-BINARY', transform=Constant())\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:850]\n",
    "test_dataset = dataset[850:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(dataset.num_node_features, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(dataset.num_node_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "        self.lin = Linear(hidden_channels * heads, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)    \n",
    "        return x\n",
    "        \n",
    "\n",
    "gcn_model = GCN(hidden_channels=64)\n",
    "gin_model = GIN(hidden_channels=64)\n",
    "gatv2_model = GATv2(hidden_channels=64, heads=8)\n",
    "\n",
    "models = [gcn_model, gin_model, gatv2_model]\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         loss = criterion(out, data.y)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(model, loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)\n",
    "         correct += int((pred == data.y).sum())\n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        train(model)\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPSConvNet\n",
      "Epoch: 010, Train Acc: 0.7588, Test Acc: 0.5667\n",
      "Epoch: 020, Train Acc: 0.7682, Test Acc: 0.7333\n"
     ]
    }
   ],
   "source": [
    "class GPSConvNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1, dropout=0.0, act='relu'):\n",
    "        super(GPSConvNet, self).__init__()\n",
    "        self.GPSConvs = nn.ModuleList()\n",
    "        h = hidden_channels[0]\n",
    "        self.preprocess = nn.Sequential(Linear(dataset.num_node_features, 2 * h), nn.GELU(), Linear(2 * h, h), nn.GELU())\n",
    "        for h in hidden_channels:\n",
    "            gatv2_conv = GATv2Conv(h, h // heads, heads=heads, dropout=dropout)\n",
    "            gps_conv = GPSConv(h, gatv2_conv, heads=4, dropout=dropout, act=act)\n",
    "            self.GPSConvs.append(gps_conv)  \n",
    "        self.final_lin = Linear(hidden_channels[-1], dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.preprocess(x)\n",
    "        for gps_conv in self.GPSConvs:\n",
    "            x = x.float()\n",
    "            x = F.relu(gps_conv(x, edge_index))\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.final_lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "hidden_channels = [64, 64, 64]\n",
    "dataset = TUDataset(root='.', name='IMDB-BINARY', transform=OneHotDegree(max_degree=140))\n",
    "data = dataset[0]\n",
    "\n",
    "model = GPSConvNet(hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "train_dataset = dataset[:850]\n",
    "test_dataset = dataset[850:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         loss = criterion(out, data.y)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)\n",
    "         correct += int((pred == data.y).sum())\n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Training {model.__class__.__name__}\")\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PascalVOC-SP: Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: LRGBDataset(8498):\n",
      "====================\n",
      "Number of graphs: 8498\n",
      "Number of features: 14\n",
      "Number of classes: 21\n",
      "\n",
      "Data(x=[460, 14], edge_index=[2, 2632], edge_attr=[2632, 2], y=[460])\n",
      "=============================================================\n",
      "Number of nodes: 460\n",
      "Number of edges: 2632\n",
      "Average node degree: 5.72\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Number of training graphs: 6798\n",
      "Number of test graphs: 1700\n"
     ]
    }
   ],
   "source": [
    "dataset = LRGBDataset(root='.', name='PascalVOC-SP')\n",
    "data = dataset[0]\n",
    "\n",
    "train_dataset = dataset[:7000]\n",
    "test_dataset = dataset[7000:]\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 7000\n",
      "Number of test graphs: 1498\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:7000]\n",
    "test_dataset = dataset[7000:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN\n",
      "Epoch: 000, Train Acc: 0.6995, Test Acc: 0.6950\n",
      "Epoch: 002, Train Acc: 0.6995, Test Acc: 0.6950\n",
      "Training GIN\n",
      "Epoch: 000, Train Acc: 0.6995, Test Acc: 0.6950\n",
      "Epoch: 002, Train Acc: 0.6995, Test Acc: 0.6950\n",
      "Training GATv2\n",
      "Epoch: 000, Train Acc: 0.6935, Test Acc: 0.6890\n",
      "Epoch: 002, Train Acc: 0.6656, Test Acc: 0.6602\n"
     ]
    }
   ],
   "source": [
    "dataset = LRGBDataset(root='.', name='PascalVOC-SP')\n",
    "\n",
    "train_dataset = dataset[:7000]\n",
    "test_dataset = dataset[7000:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GIN, self).__init__()\n",
    "        nn1 = torch.nn.Sequential(torch.nn.Linear(dataset.num_node_features, 16), torch.nn.ReLU(), torch.nn.Linear(16, 16))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(16)\n",
    "        nn2 = torch.nn.Sequential(torch.nn.Linear(16, dataset.num_classes))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)    \n",
    "\n",
    "class GATv2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(dataset.num_features, 8, heads=8)\n",
    "        self.conv2 = GATv2Conv(8 * 8, dataset.num_classes, heads=1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    total_nodes = 0\n",
    "    for data in test_loader:\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=-1)\n",
    "            test_correct += pred.eq(data.y).sum().item()\n",
    "            total_nodes += data.num_nodes\n",
    "    test_acc = test_correct / total_nodes\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "models = [GCN(), GIN(), GATv2()]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(3):\n",
    "        train(model, optimizer, train_loader)\n",
    "        train_acc = test(model, train_loader)\n",
    "        test_acc = test(model, test_loader)\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GPSConvNet\n"
     ]
    }
   ],
   "source": [
    "class GPSConvNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=1, dropout=0.0, act='relu'):\n",
    "        super(GPSConvNet, self).__init__()\n",
    "        self.GPSConvs = nn.ModuleList()\n",
    "        h = hidden_channels[0]\n",
    "        self.preprocess = nn.Sequential(Linear(dataset.num_node_features, 2 * h), nn.GELU(), Linear(2 * h, h), nn.GELU())\n",
    "        for h in hidden_channels:\n",
    "            gatv2_conv = GATv2Conv(h, h // heads, heads=heads, dropout=dropout)\n",
    "            gps_conv = GPSConv(h, gatv2_conv, heads=4, dropout=dropout, act=act)\n",
    "            self.GPSConvs.append(gps_conv)  \n",
    "        self.final_lin = Linear(hidden_channels[-1], dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.preprocess(x)\n",
    "        for gps_conv in self.GPSConvs:\n",
    "            x = x.float()\n",
    "            x = F.relu(gps_conv(x, edge_index))\n",
    "            x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.final_lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "hidden_channels = [64, 64, 64]\n",
    "dataset = LRGBDataset(root='.', name='PascalVOC-SP')\n",
    "data = dataset[0]\n",
    "\n",
    "model = GPSConvNet(hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "train_dataset = dataset[:850]\n",
    "test_dataset = dataset[850:]\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         loss = criterion(out, data.y)\n",
    "         loss.backward()\n",
    "         optimizer.step()\n",
    "         optimizer.zero_grad()\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)\n",
    "         correct += int((pred == data.y).sum())\n",
    "     return correct / len(loader.dataset)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(f\"Training {model.__class__.__name__}\")\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
